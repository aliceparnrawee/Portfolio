<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>House Price Analysis</title>
    <style>
        .code-block {
            background-color: #f4f4f4;
            border: 1px solid #ddd;
            padding: 10px;
            font-family: 'Courier New', Courier, monospace;
        }
        .result-block {
            background-color: #eef;
            border: 1px solid #ccd;
            padding: 10px;
            margin-top: 10px;
        }
        img {
            display: block;
            margin-left: auto;
            margin-right: auto;
            max-width: 100%;
        }
    </style>
</head>
<body>
    <h1>House Price Analysis and Prediction</h1>
    <div class="code-block">
        <h2>Python Code:</h2>
        <pre>
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
%matplotlib inline

from sklearn.model_selection import train_test_split
from sklearn import tree
from sklearn import neighbors
from sklearn.metrics import accuracy_score
from sklearn.model_selection import cross_val_score

# Load dataset
file_path = r'C:\Users\pweve\BSYS 4001\Data\American_Housing_Data_20231209.csv'
house = pd.read_csv(file_path)

# Find the shape
print("There are ", house.shape[0], " rows and ", house.shape[1], " columns")

# Visually inspect
house

<img src="https://github.com/aliceparnrawee/Portfolio/raw/main/HP_visual%20table.jpg" alt="House Data Table">
            
# Drop unnecessary columns
house = house.drop(columns = ['Zip Code','Address','Zip Code Population', 'Zip Code Density', 'County', 'Latitude', 'Longitude'])

# Drop rows with missing values
house = house.dropna()

# Find the shape after dropping rows with missing values
print("Now there are ", house.shape[0], " rows and ", house.shape[1], " columns")

# Visually inspect
house.head()

<img src="https://github.com/aliceparnrawee/Portfolio/raw/main/HP_visual%20table2.jpg" alt="Cleaned House Data Table">
            
# Define X and Y
X = house.drop(columns=["City"])
Y = house["City"]

# Convert categorical variables to numeric using one-hot encoding
X = pd.get_dummies(X)

# Split the data
X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.4, random_state=2022)

# Define the model
tree_model = tree.DecisionTreeClassifier(random_state=2022)

# Define the parameter grid
param_grid = {
    'max_depth': [5, 7, 9, 11, 13, 15, 17, 19, 21, 23, 25],
    'min_samples_split': [2, 5, 10, 12, 15],
    'min_samples_leaf': [1, 2, 4, 6, 8, 10]
}

# Setup the grid search
grid_search = GridSearchCV(estimator=tree_model, param_grid=param_grid, cv=5, n_jobs=-1, verbose=2)

# Fit the grid search to the data
grid_search.fit(X_train, Y_train)

# Print the best parameters and best score
print("Best Parameters: ", grid_search.best_params_)
print("Best Cross-Validation Score: ", grid_search.best_score_)

# Train on all the training data with best parameters
best_tree_model = grid_search.best_estimator_
best_tree_model.fit(X_train, Y_train)

# Test predictions
Y_pred = best_tree_model.predict(X_test)
Y_pred_train = best_tree_model.predict(X_train)

# Evaluate and print accuracies
print("The Cross Validation Accuracy was: ", grid_search.best_score_)
print("And the training Accuracy is: ", accuracy_score(Y_train, Y_pred_train))
print("And the test Accuracy is: ", accuracy_score(Y_test, Y_pred))

# Feature Importance
importances = best_tree_model.feature_importances_
indices = np.argsort(importances)[::-1]
features = X.columns

plt.figure(figsize=(12, 6))
plt.title("Feature Importances")
plt.bar(range(X.shape[1]), importances[indices], align="center")
plt.xticks(range(X.shape[1]), [features[i] for i in indices], rotation=90)
plt.tight_layout()
plt.show()

<img src="https://github.com/aliceparnrawee/Portfolio/raw/main/HP_feature%20importances.jpg" alt="Feature Importances">
            
# Example new house data
new_house_data = pd.DataFrame({
    'Price': [3999000.0],
    'Beds': [2],
    'Baths': [3],
    'Living Space': [1967],
    'State': ['New York'],
    'Median Household Income': [370046.0]
})

# Combine with the original dataset columns to handle missing columns
combined_data = pd.concat([house.drop(columns=["City"]), new_house_data], axis=0)

# Preprocess the combined data (one-hot encoding)
combined_data = pd.get_dummies(combined_data)

# Ensure the new data has the same columns as the training data
new_house_data_processed = combined_data.iloc[-1:].copy()

# Ensure columns match exactly
missing_cols = set(X.columns) - set(new_house_data_processed.columns)
for col in missing_cols:
    new_house_data_processed[col] = 0
new_house_data_processed = new_house_data_processed[X.columns]

# Predict the city for the new house data
predicted_city = best_tree_model.predict(new_house_data_processed)
print("Predicted City: ", predicted_city[0])
        </pre>
    </div>
    <div class="result-block">
        <h2>Results:</h2>
        <p><strong>Best Parameters:</strong> {'max_depth': 25, 'min_samples_leaf': 1, 'min_samples_split': 2}</p>
        <p><strong>Best Cross-Validation Score:</strong> 0.6999623653208988</p>
        <p><strong>Training Accuracy:</strong> 0.7247258931921458</p>
        <p><strong>Test Accuracy:</strong> 0.6997248624312156</p>
        <p><strong>Predicted City for New House:</strong> Sacramento</p>
    </div>
</body>
</html>
